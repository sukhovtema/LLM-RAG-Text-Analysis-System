{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz4bD270OQrD"
      },
      "outputs": [],
      "source": [
        "!pip install torch sentencepiece \\\n",
        "  accelerate \\\n",
        "  bitsandbytes \\\n",
        "  git+https://github.com/huggingface/transformers.git@15641892985b1d77acc74c9065c332cd7c3f7d7f \\\n",
        "  git+https://github.com/huggingface/peft.git\\\n",
        "  docx2txt \\\n",
        "  rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "Q_O9E1X9SM3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import docx2txt\n",
        "from rank_bm25 import BM25Okapi\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
      ],
      "metadata": {
        "id": "_-hs1HaiYCLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFhapQNoOMhV"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"IlyaGusev/saiga_mistral_7b\"\n",
        "DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\n",
        "DEFAULT_RESPONSE_TEMPLATE = \"<s>bot\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"\n",
        "\n",
        "\n",
        "class Conversation:\n",
        "    def __init__(\n",
        "        self,\n",
        "        message_template=DEFAULT_MESSAGE_TEMPLATE,\n",
        "        system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
        "        response_template=DEFAULT_RESPONSE_TEMPLATE\n",
        "    ):\n",
        "        self.message_template = message_template\n",
        "        self.response_template = response_template\n",
        "        self.messages = [{\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        }]\n",
        "\n",
        "    def add_user_message(self, message):\n",
        "        self.messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": message\n",
        "        })\n",
        "\n",
        "    def add_bot_message(self, message):\n",
        "        self.messages.append({\n",
        "            \"role\": \"bot\",\n",
        "            \"content\": message\n",
        "        })\n",
        "\n",
        "    def get_prompt(self, tokenizer):\n",
        "        final_text = \"\"\n",
        "        for message in self.messages:\n",
        "            message_text = self.message_template.format(**message)\n",
        "            final_text += message_text\n",
        "        final_text += DEFAULT_RESPONSE_TEMPLATE\n",
        "        return final_text.strip()\n",
        "\n",
        "\n",
        "def generate(model, tokenizer, prompt, generation_config):\n",
        "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
        "    data = {k: v.to(model.device) for k, v in data.items()}\n",
        "    output_ids = model.generate(\n",
        "        **data,\n",
        "        generation_config=generation_config\n",
        "    )[0]\n",
        "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
        "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "    return output.strip()\n",
        "\n",
        "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "print(generation_config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(text, max_words_per_split):\n",
        "    splits = []\n",
        "    words = text.split()\n",
        "    for i in range(0, len(words), max_words_per_split):\n",
        "        split = ' '.join(words[i:i+max_words_per_split])\n",
        "        splits.append(split)\n",
        "    return splits\n",
        "\n",
        "def read_docx_files_from_folder(folder_path):\n",
        "    texts = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.docx'):\n",
        "            docx_path = os.path.join(folder_path, filename)\n",
        "            text = docx2txt.process(docx_path)\n",
        "            texts.append(text)\n",
        "    return texts\n",
        "\n",
        "def get_top_matching_texts(folder_path, query, max_words_per_split=100, top_n=3):\n",
        "    all_splits = []\n",
        "    for text in read_docx_files_from_folder(folder_path):\n",
        "        splits = split_text(text, max_words_per_split)\n",
        "        all_splits.extend(splits)\n",
        "\n",
        "    tokenized_texts = [text.split() for text in all_splits]\n",
        "\n",
        "    bm25_model = BM25Okapi(tokenized_texts)\n",
        "\n",
        "    tokenized_query = query.split(' ')\n",
        "\n",
        "    scores = bm25_model.get_scores(tokenized_query)\n",
        "\n",
        "    results = [(score, text) for score, text in zip(scores, all_splits)]\n",
        "\n",
        "    sorted_results = sorted(results, reverse=True)\n",
        "\n",
        "    top_results = sorted_results[:top_n]\n",
        "\n",
        "    prompts = []\n",
        "    for idx, (score, text) in enumerate(top_results, start=1):\n",
        "        prompt = f\"{text}\"\n",
        "        prompts.append(prompt)\n",
        "\n",
        "    return prompts"
      ],
      "metadata": {
        "id": "zcVVWe0VRyjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/data'\n",
        "query = \"Представь, что ты - специаллист по игре спортивная мафия. Что значит термин выставлять под последние руки?\"\n",
        "top_matching_texts = get_top_matching_texts(folder_path, query)"
      ],
      "metadata": {
        "id": "fmATvV7EBA6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# without RAG\n",
        "inputs = [f\"{query}\"]\n",
        "for inp in inputs:\n",
        "    conversation = Conversation()\n",
        "    conversation.add_user_message(inp)\n",
        "    prompt = conversation.get_prompt(tokenizer)\n",
        "\n",
        "    output = generate(model, tokenizer, prompt, generation_config)\n",
        "    print(prompt)\n",
        "    print(output)\n",
        "    print()\n",
        "    print(\"==============================\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "h2c6wqHAcfL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jflXz_W_Ogg3"
      },
      "outputs": [],
      "source": [
        "# with RAG\n",
        "inputs = [f\"{top_matching_texts[0]}, {top_matching_texts[1]}, {top_matching_texts[2]}\\nuser:{query}\\nbot: Вот ответ на ваш вопрос длиной не более 400 слов:\"]\n",
        "for inp in inputs:\n",
        "    conversation = Conversation()\n",
        "    conversation.add_user_message(inp)\n",
        "    prompt = conversation.get_prompt(tokenizer)\n",
        "\n",
        "    output = generate(model, tokenizer, prompt, generation_config)\n",
        "    print(prompt)\n",
        "    print(output)\n",
        "    print()\n",
        "    print(\"==============================\")\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}